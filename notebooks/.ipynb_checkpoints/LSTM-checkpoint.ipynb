{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6552e68ef201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Embedding, LSTM, Dense, Flatten, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def lstm done\n"
     ]
    }
   ],
   "source": [
    "def get_lstm_feats(a=20000,b=10,c=300,bat=32):\n",
    "    # return train pred prob and test pred prob \n",
    "    NUM_WORDS = a\n",
    "    N = b\n",
    "    MAX_LEN = c\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/lstm.h5'\n",
    "    \n",
    "    X = train_df['text']\n",
    "    Y = train_df['author']\n",
    "    X_test = test_df['text']\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "\n",
    "    train_x = tokenizer.texts_to_sequences(X)\n",
    "    train_x = pad_sequences(train_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    test_x = tokenizer.texts_to_sequences(X_test)\n",
    "    test_x = pad_sequences(test_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(Y)\n",
    "\n",
    "    train_y = lb.transform(Y)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "    model.add(LSTM(N, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(N))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    np.random.seed(42)\n",
    "    model.fit(train_x, train_y, \n",
    "              validation_split=0.1,\n",
    "              batch_size=bat, epochs=10, \n",
    "              verbose=2,\n",
    "              callbacks=[model_chk],\n",
    "              shuffle=False\n",
    "             )\n",
    "    \n",
    "#     model = load_model(MODEL_P)\n",
    "#     train_pred = model.predict(train_x)\n",
    "#     test_pred = model.predict(test_x)\n",
    "    del model\n",
    "    gc.collect()\n",
    "#     print(log_loss(train_y,train_pred))\n",
    "#     return train_pred,test_pred\n",
    "\n",
    "print('def lstm done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 12)           192000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 12)                1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 39        \n",
      "=================================================================\n",
      "Total params: 193,395\n",
      "Trainable params: 193,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.04611, saving model to /tmp/lstm.h5\n",
      " - 21s - loss: 1.0785 - acc: 0.4030 - val_loss: 1.0461 - val_acc: 0.4234\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.04611 to 0.83449, saving model to /tmp/lstm.h5\n",
      " - 19s - loss: 0.9444 - acc: 0.5587 - val_loss: 0.8345 - val_acc: 0.6149\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.83449 to 0.71567, saving model to /tmp/lstm.h5\n",
      " - 20s - loss: 0.7279 - acc: 0.6879 - val_loss: 0.7157 - val_acc: 0.7002\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.71567 to 0.59274, saving model to /tmp/lstm.h5\n",
      " - 22s - loss: 0.5414 - acc: 0.7905 - val_loss: 0.5927 - val_acc: 0.7676\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.59274 to 0.52792, saving model to /tmp/lstm.h5\n",
      " - 23s - loss: 0.3863 - acc: 0.8606 - val_loss: 0.5279 - val_acc: 0.8049\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.52792 to 0.50380, saving model to /tmp/lstm.h5\n",
      " - 21s - loss: 0.2872 - acc: 0.9001 - val_loss: 0.5038 - val_acc: 0.8187\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 21s - loss: 0.2183 - acc: 0.9252 - val_loss: 0.5100 - val_acc: 0.8279\n",
      "Epoch 8/10\n"
     ]
    }
   ],
   "source": [
    "get_lstm_feats(16000,12,300,256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
