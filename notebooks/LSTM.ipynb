{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauranechita/miniconda3/envs/mlp/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/lauranechita/miniconda3/envs/mlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Embedding, LSTM, Dense, Flatten, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def lstm done\n"
     ]
    }
   ],
   "source": [
    "def get_lstm_feats(a=20000,b=10,c=300,bat=32):\n",
    "    # return train pred prob and test pred prob \n",
    "    NUM_WORDS = a\n",
    "    N = b\n",
    "    MAX_LEN = c\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/lstm.h5'\n",
    "    \n",
    "    X = train_df['text']\n",
    "    Y = train_df['author']\n",
    "    X_test = test_df['text']\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "\n",
    "    train_x = tokenizer.texts_to_sequences(X)\n",
    "    train_x = pad_sequences(train_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    test_x = tokenizer.texts_to_sequences(X_test)\n",
    "    test_x = pad_sequences(test_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(Y)\n",
    "\n",
    "    train_y = lb.transform(Y)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "    model.add(LSTM(N, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(N))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    np.random.seed(42)\n",
    "    model.fit(train_x, train_y, \n",
    "              validation_split=0.1,\n",
    "              batch_size=bat, epochs=10, \n",
    "              verbose=2,\n",
    "              callbacks=[model_chk],\n",
    "              shuffle=False\n",
    "             )\n",
    "    \n",
    "#     model = load_model(MODEL_P)\n",
    "#     train_pred = model.predict(train_x)\n",
    "#     test_pred = model.predict(test_x)\n",
    "    del model\n",
    "    gc.collect()\n",
    "#     print(log_loss(train_y,train_pred))\n",
    "#     return train_pred,test_pred\n",
    "\n",
    "print('def lstm done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 12)           192000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 12)                1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 39        \n",
      "=================================================================\n",
      "Total params: 193,395\n",
      "Trainable params: 193,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03576, saving model to /tmp/lstm.h5\n",
      " - 20s - loss: 1.0752 - acc: 0.4045 - val_loss: 1.0358 - val_acc: 0.4627\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03576 to 0.80886, saving model to /tmp/lstm.h5\n",
      " - 18s - loss: 0.9180 - acc: 0.5841 - val_loss: 0.8089 - val_acc: 0.6410\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.80886 to 0.64566, saving model to /tmp/lstm.h5\n",
      " - 17s - loss: 0.6648 - acc: 0.7303 - val_loss: 0.6457 - val_acc: 0.7518\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.64566 to 0.51075, saving model to /tmp/lstm.h5\n",
      " - 18s - loss: 0.4545 - acc: 0.8275 - val_loss: 0.5107 - val_acc: 0.8064\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51075 to 0.48189, saving model to /tmp/lstm.h5\n",
      " - 18s - loss: 0.3064 - acc: 0.8952 - val_loss: 0.4819 - val_acc: 0.8146\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 19s - loss: 0.2237 - acc: 0.9236 - val_loss: 0.4967 - val_acc: 0.8156\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 17s - loss: 0.1765 - acc: 0.9402 - val_loss: 0.5238 - val_acc: 0.8166\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 18s - loss: 0.1434 - acc: 0.9507 - val_loss: 0.5651 - val_acc: 0.8161\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 18s - loss: 0.1167 - acc: 0.9611 - val_loss: 0.6108 - val_acc: 0.8172\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 22s - loss: 0.1030 - acc: 0.9664 - val_loss: 0.6373 - val_acc: 0.8172\n"
     ]
    }
   ],
   "source": [
    "get_lstm_feats(16000,12,300,256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
